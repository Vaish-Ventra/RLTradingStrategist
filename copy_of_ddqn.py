# -*- coding: utf-8 -*-
"""Copy of DDQN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eBHkzEv6yYV9ZmfNsRT-eCjJTVvR5tiJ
"""

!pip install stable-baselines3[extra] sb3-contrib --quiet

from stable_baselines3 import PPO, A2C, DQN
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
from stable_baselines3.common.monitor import Monitor
from sb3_contrib import QRDQN
from sb3_contrib import RecurrentPPO
from sb3_contrib.ppo_recurrent.policies import RecurrentActorCriticPolicy

!pip install gymnasium

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler

# --- Config ---
window_size = 10
hidden_size = 64
n_epochs = 100
lr = 1e-3

# --- Load & Normalize CSV ---
df = pd.read_csv("/content/BTC_DATA (1).csv")
df.dropna(inplace=True)
df.reset_index(drop=True, inplace=True)
print(df.head())
df = df[['low', 'high', 'BB_upper', 'MOM10', 'BB_lower', 'Low14', 'volume']]  # select 7 columns
scaler = StandardScaler()
normalized_data = scaler.fit_transform(df.values)  # (N, 7)

# --- Create Sliding Window Sequences ---
X_seq = []
for i in range(window_size - 1, len(normalized_data)):
    window = normalized_data[i - window_size + 1 : i + 1]  # (window_size, 7)
    X_seq.append(window)

X_seq = np.stack(X_seq)  # (num_sequences, window_size, 7)
input_tensor = torch.tensor(X_seq, dtype=torch.float32)  # (B, T, F)

# --- RNN Encoder ---
class RNNEncoder(nn.Module):
    def __init__(self, input_size=7, hidden_size=64):
        super(RNNEncoder, self).__init__()
        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)

    def forward(self, x):
        _, h_n = self.rnn(x)  # h_n: (1, B, H)
        return h_n[-1]        # (B, H)

# --- RNN Decoder ---
class RNNDecoder(nn.Module):
    def __init__(self, hidden_size=64, output_size=7, seq_len=window_size):
        super(RNNDecoder, self).__init__()
        self.seq_len = seq_len
        self.rnn = nn.RNN(hidden_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, encoded):
        repeated = encoded.unsqueeze(1).repeat(1, self.seq_len, 1)  # (B, T, H)
        out, _ = self.rnn(repeated)                                 # (B, T, H)
        return self.fc(out)                                         # (B, T, F)

# --- Initialize Model, Loss, Optimizer ---
encoder = RNNEncoder(input_size=7, hidden_size=hidden_size)
decoder = RNNDecoder(hidden_size=hidden_size, output_size=7, seq_len=window_size)

criterion = nn.MSELoss()
optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=lr)

# --- Training Loop ---
for epoch in range(n_epochs):
    encoder.train()
    decoder.train()

    optimizer.zero_grad()
    encoded = encoder(input_tensor)                # (B, H)
    reconstructed = decoder(encoded)               # (B, T, F)
    loss = criterion(reconstructed, input_tensor)  # MSE loss
    loss.backward()
    optimizer.step()

    print(f"Epoch {epoch+1}/{n_epochs}, Loss: {loss.item():.4f}")

import torch
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler

def generate_embeddings_from_df(df, encoder, window_size=10):
    """
    Given a DataFrame and a trained encoder, return a new DataFrame of embeddings.
    Each embedding corresponds to a sliding window over `window_size` rows.
    """
    # --- Preprocess ---
    scaler = StandardScaler()
    df_scaled = scaler.fit_transform(df.values)  # normalize
    X_seq = []

    for i in range(window_size - 1, len(df_scaled)):
        window = df_scaled[i - window_size + 1 : i + 1]  # shape: (window_size, num_features)
        X_seq.append(window)

    X_seq = np.stack(X_seq)  # shape: (N - window_size + 1, window_size, num_features)
    input_tensor = torch.tensor(X_seq, dtype=torch.float32)

    # --- Encode ---
    encoder.eval()
    with torch.no_grad():
        encoded = encoder(input_tensor)  # shape: (N - window_size + 1, hidden_size)

    # --- Convert to DataFrame ---
    embed_array = encoded.numpy()  # (N - window_size + 1, hidden_size)
    embed_df = pd.DataFrame(embed_array, columns=[f"embed_{i}" for i in range(embed_array.shape[1])])

    return embed_df

# Assuming you've already trained your encoder, and have your original df:
df = pd.read_csv("/content/BTC_DATA (1).csv")
df.dropna(inplace=True)
df.reset_index(drop=True, inplace=True)
df = df[['low', 'high', 'BB_upper', 'MOM10', 'BB_lower', 'Low14', 'volume']]

# Now generate embeddings
embedded_df = generate_embeddings_from_df(df, encoder, window_size=10)

# Optional: Save or inspect
print(embedded_df.tail())
# embedded_df.to_csv("btc_embeddings.csv", index=False)

"""BTC Trading Env 1- no emebedding!"""

import gymnasium as gym
import numpy as np
import pandas as pd
from gymnasium import spaces


class BTCTradingEnv(gym.Env):
    def __init__(self, df, initial_balance=100000, window_size=50, render_mode=None):
        super().__init__()
        self.df = df.reset_index(drop=True)
        self.initial_balance = initial_balance
        self.window_size = window_size
        self.render_mode = render_mode

        # Action space: 0 = Hold, 1 = Buy, 2 = Sell
        self.action_space = spaces.Discrete(3)

        # Observation: OHLCV + indicators + balance + holdings
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, shape=(7,), dtype=np.float32
        )

    def reset(self, seed=None, options=None):
        self.balance = self.initial_balance
        self.holdings = 0
        self.current_step = self.window_size
        self.net_worth = self.initial_balance
        self.prev_net_worth = self.net_worth
        self.net_worth_history = [self.net_worth]
        self.return_history = []
        self.prev_action = 0
        return self._next_observation(), {}

    def _next_observation(self):
        data = self.df.loc[self.current_step, ['low', 'high', 'BB_upper', 'MOM10', 'BB_lower', 'Low14', 'volume']].values
        obs = np.array(list(data) + [self.balance, self.holdings], dtype=np.float32)
        return obs

    def step(self, action):
        price = self.df.loc[self.current_step, 'close']

        # Execute trade
        if action == 1 and self.balance >= price:  # Buy
            self.holdings += 1
            self.balance -= price
        elif action == 2:  # Sell
            self.holdings -= 1
            self.balance += price
        # else: Hold

        self.current_step += 1
        done = self.current_step >= len(self.df) - 1

        # Update net worth
        self.net_worth = self.balance + self.holdings * price
        self.net_worth_history.append(self.net_worth)

        # --- Compute Reward ---
        profit_reward = (self.net_worth - self.prev_net_worth) / self.initial_balance  # Scale to initial capital
        profit_reward *= 100  # Amplify the signal

        self.prev_net_worth = self.net_worth

        # Daily return
        if len(self.net_worth_history) > 1:
            daily_return = self.net_worth_history[-1] / self.net_worth_history[-2] - 1
            self.return_history.append(daily_return)

        volatility_penalty = np.std(self.return_history[-self.window_size:]) if len(self.return_history) >= self.window_size else 0
        volatility_penalty *= 10  # Amplify to be meaningful

        # Drawdown penalty
        peak = max(self.net_worth_history)
        drawdown = (self.net_worth - peak) / (peak + 1e-9)
        drawdown_penalty = abs(drawdown) if drawdown < 0 else 0
        drawdown_penalty *= 50  # Strong penalty for large drops

        # Trade penalty
        trade_penalty = 1.0 if action != self.prev_action else 0  # Now actually meaningful
        self.prev_action = action

        # Final reward (tuned weights)
        reward = (
            + 1.0 * profit_reward         # main driver
            - 0.3 * volatility_penalty    # mild penalty
            - 0.5 * drawdown_penalty      # stronger penalty
            - 0.05 * trade_penalty        # discourage flip-flopping
        )

        return self._next_observation(), reward, done, False, {}

    def render(self):
        if self.render_mode == "human":
            print(f"Step: {self.current_step}")
            print(f"Balance: {self.balance}, Holdings: {self.holdings}, Net Worth: {self.net_worth}")

"""BTC Trading Env 2- uses embeddings"""

class BTCTradingEnv2(gym.Env):
    def __init__(self, df, embedded_df, initial_balance=100000, window_size=50, render_mode=None):
        super().__init__()
        self.df = df.reset_index(drop=True)
        self.embedded_df = embedded_df.reset_index(drop=True)
        self.initial_balance = initial_balance
        self.window_size = window_size
        self.render_mode = render_mode

        # Action space: 0 = Hold, 1 = Buy, 2 = Sell
        self.action_space = spaces.Discrete(3)

        # Observation: OHLCV + indicators + balance + holdings
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, shape=(66,), dtype=np.float32
        )

    def reset(self, seed=None, options=None):
        self.balance = self.initial_balance
        self.holdings = 0
        self.current_step = self.window_size
        self.net_worth = self.initial_balance
        self.prev_net_worth = self.net_worth
        self.net_worth_history = [self.net_worth]
        self.return_history = []
        self.prev_action = 0
        return self._next_observation(), {}

    def _next_observation(self):
        data = self.embedded_df.loc[self.current_step].values
        obs = np.array(list(data) + [self.balance, self.holdings], dtype=np.float32)
        return obs

    def step(self, action):
        price = self.df.loc[self.current_step, 'close']

        # Execute trade
        if action == 1 and self.balance >= price:  # Buy
            self.holdings += 1
            self.balance -= price
        elif action == 2:  # Sell
            self.holdings -= 1
            self.balance += price
        # else: Hold

        self.current_step += 1
        done = self.current_step >= len(self.embedded_df) - 1
        if done!=0:
          print("DONE")

        # Update net worth
        self.net_worth = self.balance + self.holdings * price
        self.net_worth_history.append(self.net_worth)

        # --- Compute Reward ---
        profit_reward = (self.net_worth - self.prev_net_worth) / self.initial_balance  # Scale to initial capital
        profit_reward *= 100  # Amplify the signal

        self.prev_net_worth = self.net_worth

        # Daily return
        if len(self.net_worth_history) > 1:
            daily_return = self.net_worth_history[-1] / self.net_worth_history[-2] - 1
            self.return_history.append(daily_return)

        volatility_penalty = np.std(self.return_history[-self.window_size:]) if len(self.return_history) >= self.window_size else 0
        volatility_penalty *= 10  # Amplify to be meaningful

        # Drawdown penalty
        peak = max(self.net_worth_history)
        drawdown = (self.net_worth - peak) / (peak + 1e-9)
        drawdown_penalty = abs(drawdown) if drawdown < 0 else 0
        drawdown_penalty *= 50  # Strong penalty for large drops

        # Trade penalty
        trade_penalty = 1.0 if action != self.prev_action else 0  # Now actually meaningful
        self.prev_action = action

        # Final reward (tuned weights)
        reward = (
            + 1.0 * profit_reward         # main driver
            - 0.3 * volatility_penalty    # mild penalty
            - 0.5 * drawdown_penalty      # stronger penalty
            - 0.05 * trade_penalty        # discourage flip-flopping
        )

        return self._next_observation(), reward, done, False, {}

    def render(self):
        if self.render_mode == "human":
            print(f"Step: {self.current_step}")
            print(f"Balance: {self.balance}, Holdings: {self.holdings}, Net Worth: {self.net_worth}")

!pip install gym pandas numpy matplotlib torch

"""DDQN Training- Random"""

import gym
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
from collections import deque
import random


# --- Q-Network ---
class QNetwork(nn.Module):
    def __init__(self, input_dim, n_actions):
        super(QNetwork, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, n_actions)
        )

    def forward(self, x):
        return self.net(x)

# --- Replay Buffer ---
class ReplayBuffer:
    def __init__(self, capacity):
        self.buffer = deque(maxlen=capacity)

    def add(self, transition):
        self.buffer.append(transition)

    #random sampling
    def sample(self, batch_size):
        batch = random.sample(self.buffer, batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)
        return (
            torch.tensor(states, dtype=torch.float32),
            torch.tensor(actions, dtype=torch.int64),
            torch.tensor(rewards, dtype=torch.float32),
            torch.tensor(next_states, dtype=torch.float32),
            torch.tensor(dones, dtype=torch.float32)
        )


    #def sample(self, batch_size):
    #    if len(self.buffer) < batch_size:
    #        batch = list(self.buffer)
    #    else:
    #        batch = list(self.buffer)[-batch_size:]

    #   states, actions, rewards, next_states, dones = zip(*batch)
    #    return (
    #        torch.tensor(states, dtype=torch.float32),
    #        torch.tensor(actions, dtype=torch.int64),
    #        torch.tensor(rewards, dtype=torch.float32),
    #        torch.tensor(next_states, dtype=torch.float32),
    #        torch.tensor(dones, dtype=torch.float32)
    #   )


    def __len__(self):
        return len(self.buffer)

# --- DDQN Agent ---
class DDQNAgent:
    def __init__(self, input_dim, n_actions, gamma=0.99, lr=1e-3, epsilon=1.0, eps_min=0.01,
                 eps_decay=1e-4, buffer_size=100000, batch_size=64, target_update=100):
        self.n_actions = n_actions
        self.gamma = gamma
        self.lr = lr
        self.epsilon = epsilon
        self.eps_min = eps_min
        self.eps_decay = eps_decay
        self.batch_size = batch_size
        self.target_update = target_update
        self.learn_step = 0

        self.q_eval = QNetwork(input_dim, n_actions)
        self.q_target = QNetwork(input_dim, n_actions)
        self.q_target.load_state_dict(self.q_eval.state_dict())
        self.q_target.eval()

        self.optimizer = optim.Adam(self.q_eval.parameters(), lr=lr)
        self.loss = nn.MSELoss()
        self.memory = ReplayBuffer(buffer_size)

    def choose_action(self, state):
        if np.random.rand() < self.epsilon:
            return np.random.randint(self.n_actions)
        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)
        with torch.no_grad():
            q_values = self.q_eval(state_tensor)
        return torch.argmax(q_values).item()

    def store_transition(self, s, a, r, s_, done):
        self.memory.add((s, a, r, s_, done))

    def learn(self):
        if len(self.memory) < self.batch_size:
            return

        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)

        q_pred = self.q_eval(states).gather(1, actions.unsqueeze(1)).squeeze(1)

        next_actions = self.q_eval(next_states).argmax(1)
        q_next = self.q_target(next_states).gather(1, next_actions.unsqueeze(1)).squeeze(1)
        q_target = rewards + self.gamma * q_next * (1 - dones)

        loss = self.loss(q_pred, q_target)

        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        self.learn_step += 1
        if self.learn_step % self.target_update == 0:
            self.q_target.load_state_dict(self.q_eval.state_dict())

        self.epsilon = max(self.eps_min, self.epsilon - self.eps_decay)

# --- Training ---
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

def train_ddqn(df, embedded_df, n_episodes=300):
    env = BTCTradingEnv2(df, embedded_df)
    agent = DDQNAgent(input_dim=66, n_actions=3)
    reward_history = []  # Step 1: Initialize reward log

    for ep in range(n_episodes):
        state, _ = env.reset()
        done = False
        total_reward = 0
        episodelen=0

        while not done:
            action = agent.choose_action(state)
            next_state, reward, done, _, _ = env.step(action)
            agent.store_transition(state, action, reward, next_state, done)
            agent.learn()
            state = next_state
            total_reward += reward
            episodelen+=1
            if episodelen>=500:
              break

        reward_history.append(total_reward)  # Step 1: Save reward
        mean_reward = np.mean(reward_history[-10:])  # Rolling mean (last 10)

        print(f"Episode {ep+1}/{n_episodes} - Reward: {total_reward:.2f}, "
              f"Mean(Last 10): {mean_reward:.2f}, Epsilon: {agent.epsilon:.3f}")

    # Step 2: Optionally save reward history
    np.save("reward_history.npy", np.array(reward_history))

    # Step 3: Plot reward curve
    plt.figure(figsize=(10, 5))
    plt.plot(reward_history, label="Episode Reward")
    plt.plot(pd.Series(reward_history).rolling(10).mean(), label="Mean (10)", linestyle='--')
    plt.xlabel("Episode")
    plt.ylabel("Total Reward")
    plt.title("DDQN Training Reward Trend")
    plt.legend()
    plt.grid()
    plt.tight_layout()
    plt.show()

    return agent


def train_ddqn_init(df, n_episodes=300):
    env = BTCTradingEnv(df)
    agent = DDQNAgent(input_dim=9, n_actions=3)
    reward_history = []  # Step 1: Initialize reward log

    for ep in range(n_episodes):
        state, _ = env.reset()
        done = False
        total_reward = 0
        episodelen=0

        while not done:
            action = agent.choose_action(state)
            next_state, reward, done, _, _ = env.step(action)
            agent.store_transition(state, action, reward, next_state, done)
            agent.learn()
            state = next_state
            total_reward += reward
            episodelen+=1
            if episodelen>=500:
              break

        reward_history.append(total_reward)  # Step 1: Save reward
        mean_reward = np.mean(reward_history[-10:])  # Rolling mean (last 10)

        print(f"Episode {ep+1}/{n_episodes} - Reward: {total_reward:.2f}, "
              f"Mean(Last 10): {mean_reward:.2f}, Epsilon: {agent.epsilon:.3f}")

    # Step 2: Optionally save reward history
    np.save("reward_history.npy", np.array(reward_history))

    # Step 3: Plot reward curve
    plt.figure(figsize=(10, 5))
    plt.plot(reward_history, label="Episode Reward")
    plt.plot(pd.Series(reward_history).rolling(10).mean(), label="Mean (10)", linestyle='--')
    plt.xlabel("Episode")
    plt.ylabel("Total Reward")
    plt.title("DDQN Training Reward Trend")
    plt.legend()
    plt.grid()
    plt.tight_layout()
    plt.show()

    return agent

"""DDQN Training- Sequantial (IGNORE)"""

import gym
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
from collections import deque
import random


# --- Q-Network ---
class QNetwork(nn.Module):
    def __init__(self, input_dim, n_actions):
        super(QNetwork, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, n_actions)
        )

    def forward(self, x):
        return self.net(x)

# --- Replay Buffer ---
class ReplayBuffer:
    def __init__(self, capacity):
        self.buffer = deque(maxlen=capacity)

    def add(self, transition):
        self.buffer.append(transition)

    #random sampling
    ##def sample(self, batch_size):
    ##    batch = random.sample(self.buffer, batch_size)
    ##    states, actions, rewards, next_states, dones = zip(*batch)
    ##    return (
    ##        torch.tensor(states, dtype=torch.float32),
    ##        torch.tensor(actions, dtype=torch.int64),
    ##        torch.tensor(rewards, dtype=torch.float32),
    ##        torch.tensor(next_states, dtype=torch.float32),
    ##        torch.tensor(dones, dtype=torch.float32)
    ##    )


    def sample(self, batch_size):
        if len(self.buffer) < batch_size:
            batch = list(self.buffer)
        else:
            batch = list(self.buffer)[-batch_size:]

        states, actions, rewards, next_states, dones = zip(*batch)
        return (
            torch.tensor(states, dtype=torch.float32),
            torch.tensor(actions, dtype=torch.int64),
            torch.tensor(rewards, dtype=torch.float32),
            torch.tensor(next_states, dtype=torch.float32),
            torch.tensor(dones, dtype=torch.float32)
       )


    def __len__(self):
        return len(self.buffer)

# --- DDQN Agent ---
class DDQNAgent:
    def __init__(self, input_dim, n_actions, gamma=0.99, lr=1e-3, epsilon=1.0, eps_min=0.01,
                 eps_decay=1e-4, buffer_size=100000, batch_size=64, target_update=100):
        self.n_actions = n_actions
        self.gamma = gamma
        self.lr = lr
        self.epsilon = epsilon
        self.eps_min = eps_min
        self.eps_decay = eps_decay
        self.batch_size = batch_size
        self.target_update = target_update
        self.learn_step = 0

        self.q_eval = QNetwork(input_dim, n_actions)
        self.q_target = QNetwork(input_dim, n_actions)
        self.q_target.load_state_dict(self.q_eval.state_dict())
        self.q_target.eval()

        self.optimizer = optim.Adam(self.q_eval.parameters(), lr=lr)
        self.loss = nn.MSELoss()
        self.memory = ReplayBuffer(buffer_size)

    def choose_action(self, state):
        if np.random.rand() < self.epsilon:
            return np.random.randint(self.n_actions)
        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)
        with torch.no_grad():
            q_values = self.q_eval(state_tensor)
        return torch.argmax(q_values).item()

    def store_transition(self, s, a, r, s_, done):
        self.memory.add((s, a, r, s_, done))

    def learn(self):
        if len(self.memory) < self.batch_size:
            return

        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)

        q_pred = self.q_eval(states).gather(1, actions.unsqueeze(1)).squeeze(1)

        next_actions = self.q_eval(next_states).argmax(1)
        q_next = self.q_target(next_states).gather(1, next_actions.unsqueeze(1)).squeeze(1)
        q_target = rewards + self.gamma * q_next * (1 - dones)

        loss = self.loss(q_pred, q_target)

        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        self.learn_step += 1
        if self.learn_step % self.target_update == 0:
            self.q_target.load_state_dict(self.q_eval.state_dict())

        self.epsilon = max(self.eps_min, self.epsilon - self.eps_decay)

# --- Training ---
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

def train_ddqn(df, embedded_df, n_episodes=300):
    env = BTCTradingEnv2(df, embedded_df)
    agent = DDQNAgent(input_dim=66, n_actions=3)
    reward_history = []  # Step 1: Initialize reward log

    for ep in range(n_episodes):
        state, _ = env.reset()
        done = False
        total_reward = 0
        episodelen=0

        while not done:
            action = agent.choose_action(state)
            next_state, reward, done, _, _ = env.step(action)
            agent.store_transition(state, action, reward, next_state, done)
            agent.learn()
            state = next_state
            total_reward += reward
            episodelen+=1
            if episodelen>=500:
              break

        reward_history.append(total_reward)  # Step 1: Save reward
        mean_reward = np.mean(reward_history[-10:])  # Rolling mean (last 10)

        print(f"Episode {ep+1}/{n_episodes} - Reward: {total_reward:.2f}, "
              f"Mean(Last 10): {mean_reward:.2f}, Epsilon: {agent.epsilon:.3f}")

    # Step 2: Optionally save reward history
    np.save("reward_history.npy", np.array(reward_history))

    # Step 3: Plot reward curve
    plt.figure(figsize=(10, 5))
    plt.plot(reward_history, label="Episode Reward")
    plt.plot(pd.Series(reward_history).rolling(10).mean(), label="Mean (10)", linestyle='--')
    plt.xlabel("Episode")
    plt.ylabel("Total Reward")
    plt.title("DDQN Training Reward Trend")
    plt.legend()
    plt.grid()
    plt.tight_layout()
    plt.show()

    return agent

"""Random- basic ddqn- no rnn (BTCTradingEnv1)"""

import pandas as pd

df = pd.read_csv("/content/BTC_DATA (1).csv")
df.dropna(inplace=True)
agent = train_ddqn_init(df, n_episodes=300)  # You can change n_episodes as needed
#random, no rnn

"""Random ddqn with rnn (BTCTradingEnv2)"""

train_ddqn(df, embedded_df, n_episodes=300)

"""RPPO using BTCTradingEnv2, ie, RNN"""

df = pd.read_csv("/content/BTC_DATA (1).csv")
df.dropna(inplace=True)
df.reset_index(drop=True, inplace=True)
print(df.head())
#df = df[['low', 'high', 'BB_upper', 'MOM10', 'BB_lower', 'Low14', 'volume']]  # select 7 columns

vec_env = DummyVecEnv([lambda: Monitor(BTCTradingEnv2(df, embedded_df))])
env= VecNormalize(vec_env, norm_obs=True, norm_reward=True)

model = RecurrentPPO(
    RecurrentActorCriticPolicy,
    env,
    verbose=1,
)
model.learn(total_timesteps=50000)

"""Inferencing models-"""

import torch

def run_ddqn_inference(agent, df, render=False, start_step=None):
    env = BTCTradingEnv(df)
    state, _ = env.reset()

    # Optional: skip to a later step
    if start_step:
        env.current_step = start_step
        state = env._next_observation()

    done = False
    total_reward = 0
    trajectory = []
    i=0

    while not done:
        action = agent.choose_action(state)
        next_state, reward, done, _, _ = env.step(action)

        if render:
            env.render()

        trajectory.append((env.current_step, action, reward, env.net_worth))
        total_reward += reward
        state = next_state
        i=i+1
        print(i, "- ", action, reward)
        if i>10:
          break

    print(f"\n[Inference Completed] Total Reward: {total_reward:.2f} | Final Net Worth: {env.net_worth:.2f}")
    return trajectory

trajectory = run_ddqn_inference(agent, df, render=True)

# --- Backtest for DDQN Agent ---
def backtest_ddqn(agent, df, start_step=None, render=False):
    """
    Backtests the DDQN agent using BTCTradingEnv.
    Returns net worths, actions taken, and performance metrics.
    """
    env = BTCTradingEnv(df)
    state, _ = env.reset()

    # Optional: Start from a specific step
    if start_step:
        env.current_step = start_step
        state = env._next_observation()

    done = False
    net_worths = []
    actions_taken = []

    while not done:
        action = agent.choose_action(state)
        next_state, reward, done, _, _ = env.step(action)

        if render:
            env.render()

        net_worths.append(env.net_worth)
        actions_taken.append(action)
        state = next_state

    perf = compute_performance_metrics(net_worths, actions_taken)
    trade_stats = analyze_trades(actions_taken, net_worths)

    return net_worths, actions_taken, {**perf, **trade_stats}

# --- Backtest for RecurrentPPO ---
def backtest_ppo(model, env, original_df, start_step=None, render=False):
    """
    Backtests a trained RecurrentPPO model using VecNormalize-wrapped BTCTradingEnv2.
    Requires access to both the original df and normalized environment.
    """
    obs = env.reset()[0]
    env.envs[0].env.current_step = start_step or env.envs[0].env.window_size
    env.envs[0].env.balance = 100000
    env.envs[0].env.holdings = 0
    env.envs[0].env.net_worth = 100000

    done = False
    net_worths = []
    actions_taken = []

    lstm_states = None
    episode_starts = np.ones((env.num_envs,), dtype=bool)

    while not done:
        action, lstm_states = model.predict(
            obs,
            state=lstm_states,
            episode_start=episode_starts,
            deterministic=True
        )

        # Ensure action is a NumPy array with a batch dimension for env.step()
        # Explicitly convert the action to an integer
        action_value = int(np.ravel(action)[0])
        action_to_step = np.array([action_value])


        obs, reward, done, info = env.step(action_to_step)
        current_step = env.envs[0].env.current_step
        if current_step >= len(original_df) - 1:
            break


        if render:
            env.envs[0].render()

        net_worths.append(env.envs[0].env.net_worth)
        actions_taken.append(action_value) # Append the integer action value

    perf = compute_performance_metrics(net_worths, actions_taken)
    trade_stats = analyze_trades(actions_taken, net_worths)

    return net_worths, actions_taken, {**perf, **trade_stats}

# --- Performance Metrics ---
def compute_performance_metrics(net_worths, actions, initial_balance=100000):
    returns = np.diff(net_worths) / net_worths[:-1]
    total_return = (net_worths[-1] - net_worths[0]) / net_worths[0] * 100

    sharpe = (np.mean(returns) / (np.std(returns) + 1e-9)) * np.sqrt(252)
    downside_returns = [r for r in returns if r < 0]
    sortino = (np.mean(returns) / (np.std(downside_returns) + 1e-9)) * np.sqrt(252)

    cumulative = np.maximum.accumulate(net_worths)
    drawdowns = (net_worths - cumulative) / (cumulative + 1e-9)
    max_drawdown = np.min(drawdowns) * 100

    volatility = np.std(returns) * np.sqrt(252) * 100

    actions = np.array(actions)
    action_counts = {
        "Hold": int(np.sum(actions == 0)),
        "Buy": int(np.sum(actions == 1)),
        "Sell": int(np.sum(actions == 2))
    }

    return {
        "Total Return (%)": total_return,
        "Sharpe Ratio": sharpe,
        "Sortino Ratio": sortino,
        "Max Drawdown (%)": max_drawdown,
        "Volatility (Annual %)": volatility,
        "Action Counts": action_counts
    }

# --- Trade Analytics ---
def analyze_trades(actions, net_worths):
    trades = []
    in_position = False
    entry_index = None
    trade_returns = []

    for i in range(1, len(actions)):
        if actions[i - 1] == 1 and not in_position:
            in_position = True
            entry_index = i
        elif actions[i - 1] == 2 and in_position:
            exit_index = i
            in_position = False
            if entry_index is not None and exit_index > entry_index:
                entry_worth = net_worths[entry_index - 1]
                exit_worth = net_worths[exit_index]
                trade_return = (exit_worth - entry_worth) / entry_worth
                trade_returns.append(trade_return)

    if len(trade_returns) == 0:
        return {
            "Total Trades": 0,
            "Win %": 0,
            "Profit Ratio": 0,
            "Avg Trade Return (%)": 0,
            "Max Trade Return (%)": 0,
            "Min Trade Return (%)": 0
        }

    wins = [r for r in trade_returns if r > 0]
    losses = [r for r in trade_returns if r <= 0]

    win_pct = 100 * len(wins) / len(trade_returns)
    avg_win = np.mean(wins) if wins else 0
    avg_loss = abs(np.mean(losses)) if losses else 1e-9
    profit_ratio = avg_win / avg_loss if avg_loss != 0 else 0

    return {
        "Total Trades": len(trade_returns),
        "Win %": win_pct,
        "Profit Ratio": profit_ratio,
        "Avg Trade Return (%)": np.mean(trade_returns) * 100,
        "Max Trade Return (%)": np.max(trade_returns) * 100,
        "Min Trade Return (%)": np.min(trade_returns) * 100
    }

# --- Plotting ---
def plot_networth_curve(net_worths, label="Strategy"):
    plt.figure(figsize=(10, 5))
    plt.plot(net_worths, label=label)
    plt.xlabel("Step")
    plt.ylabel("Net Worth")
    plt.title(f"{label} - Net Worth Over Time")
    plt.grid(True)
    plt.legend()
    plt.tight_layout()
    plt.show()

# --- DDQN Backtest ---
networths_ddqn, actions_ddqn, metrics_ddqn = backtest_ddqn(agent, df)
plot_networth_curve(networths_ddqn, label="DDQN")
print("DDQN Metrics:", metrics_ddqn)

# --- PPO Backtest ---
networths_ppo, actions_ppo, metrics_ppo = backtest_ppo(model, env, df)
plot_networth_curve(networths_ppo, label="Recurrent PPO")
print("Recurrent PPO Metrics:", metrics_ppo)